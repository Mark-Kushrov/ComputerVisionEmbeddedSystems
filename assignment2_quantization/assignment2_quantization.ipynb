{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "interpreter": {
      "hash": "12f6a1c1aae0e354ce14353100cae469b6086d18bb2c066cd594a2d3c862875e"
    },
    "kernelspec": {
      "display_name": "Python 3.9.6 64-bit ('lu-homework': conda)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "orig_nbformat": 4,
    "colab": {
      "name": "Quantization.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HhW8-Hvoc-NB"
      },
      "source": [
        "# Homework 3 - Neural Network Post-Training Static Quantization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wyWuWIqfdksv"
      },
      "source": [
        "# Setup\n",
        "> **TASKS:**\n",
        "> 1. Run these cells to grab the PyPI packages and import the dependencies for the notebook. You can click into the \"Files\" explorer on the sidebar to confirm that `./ClassyClassifierParams.pt` was appropriately downloaded."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r2q5Zis7L-Oe"
      },
      "source": [
        "!pip install torchinfo\n",
        "!pip install gdown\n",
        "!gdown https://drive.google.com/uc?id=1xrTg4FfhV_znq6g1KTThK44vbIAqEvmd -O ./ClassyClassifierParams.pt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GYDcL0okLQ3_"
      },
      "source": [
        "import copy\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import statistics\n",
        "import torch\n",
        "import torchinfo\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.quantization import get_default_qconfig\n",
        "from torch.quantization.quantize_fx import prepare_fx, convert_fx, QuantizedGraphModule\n",
        "from typing import Union, Tuple"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZkyUkzUZLQ4M"
      },
      "source": [
        "# Data\n",
        "In this assignment, you'll be working with the [CIFAR-10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html). CIFAR-10 contains 60,000 tiny images (RGB, 32x32), divided among 10 object classes (there's a \"frog\" in the dataset - that's pretty terrific). The testing subset of CIFAR-10 contains 10,000 images.\n",
        "\n",
        "> **TASKS:**\n",
        "> 1. Read the code in the cells and run them, so that you understand how the CIFAR-10 code works.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XtbMn7OoMuLt"
      },
      "source": [
        "CIFAR10_SAVE_LOCATION = \"./cifar10_dataset/\"\n",
        "CIFAR10_CLASS_NAMES = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "BATCH_SIZE = 5 # images get passed in 5 at a time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A5pQLhXaLQ4M"
      },
      "source": [
        "preprocessor = transforms.Compose(\n",
        "    [\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "    ]\n",
        ")\n",
        "\n",
        "test_dataset = torchvision.datasets.CIFAR10(root=CIFAR10_SAVE_LOCATION,\n",
        "                                            train=False,\n",
        "                                            download=True,\n",
        "                                            transform=preprocessor)\n",
        "test_dataloader = DataLoader(test_dataset,\n",
        "                             batch_size=BATCH_SIZE,\n",
        "                             shuffle=False)\n",
        "\n",
        "def display_cifar10_imgs(imgs: torch.Tensor):\n",
        "    imgs = imgs / 2 + 0.5 # CIFAR-10 images are normalized; need to de-normalize first\n",
        "    plt.imshow(np.transpose(imgs.numpy(), (1, 2, 0))) # Reorder channels\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CbkFzm7rLQ4O"
      },
      "source": [
        "dataiter = iter(test_dataloader)\n",
        "first_batch_of_imgs, first_batch_of_groundtruth_labels = dataiter.next()\n",
        "\n",
        "print(\"SAMPLE IMAGES from CIFAR-10:\")\n",
        "display_cifar10_imgs(torchvision.utils.make_grid(first_batch_of_imgs))\n",
        "print(\"CORRESPONDING GROUND TRUTH LABELS:\")\n",
        "print(\"    \" + \"     \".join([CIFAR10_CLASS_NAMES[idx] for idx in first_batch_of_groundtruth_labels]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wz5LLL-6LQ4H"
      },
      "source": [
        "# Model (5pts)\n",
        "\n",
        "We've whipped up a basic image classifier model, similar to the one from this [official PyTorch tutorial](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html). It's a relatively small network, yet it achieves reasonable accuracy (~68.7% correct predictions on CIFAR-10's 10,000 test images). We pre-trained the model for you (the weights are in the `./ClassyClassifierParams.pt` file you grabbed during the \"Setup\" step).\n",
        "\n",
        "> **TASKS:**\n",
        "> 1. Read the code in the cells and run them, so that you understand how the `ClassyClassifier` works.\n",
        "> 2. Answer the concept question. (5pts)\n",
        "\n",
        "**Concept Question 1: In `ClassyClassifier`, which layers are suitable for post-training static quantization? Why?**\n",
        "\n",
        "_[your answer here]_"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4WUSYUEFLQ4J"
      },
      "source": [
        "CLASSY_CLASSIFIER_PARAMETERS_FILENAME = \"./ClassyClassifierParams.pt\"\n",
        "\n",
        "class ClassyClassifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # INPUT SHAPE: Bx3x32x32 (B is for \"batch size,\" in this case: 5)\n",
        "        self.layer1_conv = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=5, stride=1) # output Bx16x28x28\n",
        "        self.layer2_pool = nn.MaxPool2d(kernel_size=2, stride=2) # output Bx16x14x14\n",
        "        self.layer3_conv = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=5, stride=1) # output Bx32x10x10\n",
        "        self.layer4_pool = nn.MaxPool2d(kernel_size=2, stride=2) # output Bx32x5x5\n",
        "        self.layer5_flat = nn.Flatten() # output Bx(32x5x5=800)\n",
        "        self.layer6_fc = nn.Linear(in_features=800, out_features=128) # output Bx128\n",
        "        self.layer7_fc = nn.Linear(in_features=128, out_features=84) # output Bx84\n",
        "        self.layer8_fc = nn.Linear(in_features=84, out_features=10) # output Bx10\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        x = F.relu(self.layer1_conv(x))\n",
        "        x = self.layer2_pool(x)\n",
        "        x = F.relu(self.layer3_conv(x))\n",
        "        x = self.layer4_pool(x)\n",
        "        x = self.layer5_flat(x)\n",
        "        x = F.relu(self.layer6_fc(x))\n",
        "        x = F.relu(self.layer7_fc(x))\n",
        "        x = self.layer8_fc(x)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-OvpjmooLQ4N"
      },
      "source": [
        "fp32_classifier = ClassyClassifier()\n",
        "fp32_classifier.load_state_dict(torch.load(CLASSY_CLASSIFIER_PARAMETERS_FILENAME, map_location=torch.device(\"cpu\")))\n",
        "\n",
        "print(\"SUMMARY OF ClassyClassifier\")\n",
        "print(\"    input size 1x3x32x32, the size of an image from the CIFAR-10 dataset\")\n",
        "torchinfo.summary(fp32_classifier, input_size=(1, 3, 32, 32), device=\"cpu\") # CIFAR-10 is 3x32x32"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Hij7o4iLQ4P"
      },
      "source": [
        "# Quantizing by hand (85pts)\n",
        "\n",
        "Quantization follows the following equation:\n",
        "\n",
        "`FP32 VALUE = scalefactor * (QUANT INT8 VALUE - zeropoint)`\n",
        "\n",
        "Therefore,\n",
        "\n",
        "`QUANT INT8 VALUE = FP32 VALUE/scalefactor + zeropoint` will give you the quantized value.\n",
        "\n",
        "The scalefactor and zeropoint can be solved by using algebra. Let's say you're trying to quantize a floating-point weight tensor with a min and max of A and B to an int-8 range of 0 and 255. You can plug those into the equations to solve for the scalefactor and then use substitution to calculate the zeropoint.\n",
        "\n",
        "Now, you can just save the weight tensor as a much smaller INT 8 tensor and a scalefactor and zeropoint! Refer to the lecture and assignment slides for further details.\n",
        "\n",
        "Note that this you will **NOT** be doing true quantization - ByteTensors will **NOT** be returned.\n",
        "    \n",
        "Instead, you will do \"pseudo-quantization.\" You'll do all the math with FloatTensors, but the values of the FloatTensor are integers in range [0, 255].\n",
        "\n",
        "You will calibrate the neural network by doing forward pass on images from the calibration dataset (we just reuse the test dataset for this) and then recording the layer's minimum/maximum values as the image passes through each layer. Those values will then be averaged and used to generate scalefactors and zero points.\n",
        "\n",
        "> **TASKS:**\n",
        "> 1. Make sure you understand the Quantization lecture slides. If you do, this assignment should be relatively simple!\n",
        "> 2. Read the docstrings for each function so you understand what everything is supposed to do.\n",
        "> 3. Fill out the `#TODO`s in the code below. The comments should guide you through the process; you can work top to bottom.\n",
        "> 4. Run the code, you should see that you get the exact same predictions as the original, unquantized neural network.\n",
        "\n",
        "*Grading details*\n",
        "Each properly completed #TODO (18 of them) is worth 4 points except the \"complete the forward pass\" `#TODO` which is worth 13 points. 4x18+13=85 total"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QcoKvs8eLQ4Q"
      },
      "source": [
        "## Helper functions to calculate scalefactor/zeropoint, quantize/dequantize tensors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QG-Ud7ycLQ4R"
      },
      "source": [
        "def calculate_scalefactor_and_zeropoint(min_val: float, max_val: float) -> Tuple[float, int]:\n",
        "    \"\"\"Calculates the scale factor and zero point to quantize from a [`min_val`, `max_val`] range in 32-bit float to [0, 255].\n",
        "    \n",
        "    Follows the quantization formula: FP32 VALUE = scalefactor * (QUANT INT8 VALUE - zeropoint).\n",
        "\n",
        "    Using the formula, float value `min_val` quantizes to `INT8_MIN` of 0.\n",
        "\n",
        "    Args:\n",
        "        min_val (float): Minimum value in range of interest.\n",
        "        max_val (float): Maximum value in range of interest.\n",
        "\n",
        "    Returns:\n",
        "        tuple[float, int]: Scale factor, zero point\n",
        "    \"\"\"\n",
        "    INT8_MIN = 0\n",
        "    INT8_MAX = 255\n",
        "\n",
        "    scalefactor =  # TODO: fill this in such that you can scale from [min_val, max_val] to [0, 255]\n",
        "    zeropoint =  # TODO: fill this in\n",
        "    \n",
        "    # This clamps the zero point appropriately into the range [0, 255]\n",
        "    if zeropoint < INT8_MIN:\n",
        "        zeropoint = INT8_MIN\n",
        "    elif zeropoint > INT8_MAX:\n",
        "        zeropoint = INT8_MAX\n",
        "\n",
        "    return scalefactor, int(zeropoint)\n",
        "\n",
        "\n",
        "def quantize_tensor(fp32_tensor: torch.Tensor, min_val: float, max_val: float) -> Tuple[torch.Tensor, float, int]:\n",
        "    \"\"\"Pseudo-quantizes a 32-bit float tensor with minimum of `min_val` and maximum of `max_val` to integer range [0, 255].\n",
        "\n",
        "    This is done using the quantization formula `quantized_tensor = zeropoint + fp32_tensor / scalefactor`.\n",
        "    \n",
        "    Note that this function does **NOT** truly quantize - it does **NOT** return a ByteTensor.\n",
        "    \n",
        "    It still returns a FloatTensor, but the values of the FloatTensor are integers in range [0, 255].\n",
        "\n",
        "    Args:\n",
        "        fp32_tensor (torch.Tensor): The tensor to quantize.\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: A copy of `fp32_tensor`, with all values in integer range [0, 255].\n",
        "    \"\"\"\n",
        "    INT8_MIN = 0\n",
        "    INT8_MAX = 255\n",
        "    scalefactor, zeropoint = calculate_scalefactor_and_zeropoint(min_val, max_val)\n",
        "    quantized_tensor =  # TODO: calculate this in terms of zeropoint, scalefactor, and fp32_tensor\n",
        "    quantized_tensor = quantized_tensor.clamp(INT8_MIN, INT8_MAX).round() # Clamp to [0, 255] and round to int\n",
        "    return quantized_tensor, scalefactor, int(zeropoint)\n",
        "\n",
        "\n",
        "def dequantize_tensor(int8_tensor: torch.Tensor, scalefactor: float, zeropoint: int) -> torch.Tensor:\n",
        "    \"\"\"Dequantizes a tensor represented in integer range [0, 255] to 32-bit float.\n",
        "\n",
        "    This is done by rearranging the quantization formula of `int8_tensor = zeropoint + fp32_tensor / scalefactor`.\n",
        "\n",
        "    Args:\n",
        "        int8_tensor (torch.Tensor): The quantized tensor.\n",
        "        scalefactor (float): The scale factor of the quantized tensor.\n",
        "        zeropoint (int): The zero point of the quantized tensor.\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: The dequantized, 32-bit float tensor.\n",
        "    \"\"\"\n",
        "    return  # TODO: return the quantized tensor in terms of int8_tensor, scalefactor, and zeropoint"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PrqUSez6Ihlk"
      },
      "source": [
        "## Calibration and quantization functionality"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bd76aX5uLQ4T"
      },
      "source": [
        "class QuantizedLayer():\n",
        "    \"\"\"An additional construct to manage pseudo-quantized weights and biases for Conv2d and Linear layers\n",
        "    \"\"\"\n",
        "    def __init__(self, fp32_layer: Union[nn.Conv2d, nn.Linear]) -> None:\n",
        "        \"\"\"Constructor for a QuantizedLayer for either torch.nn.Conv2d or a torch.nn.Linear with 32-bit float weights/biases.\n",
        "\n",
        "        Args:\n",
        "            fp32_layer (nn.Conv2d | nn.Linear): The layer to quantize.\n",
        "        \"\"\"\n",
        "        self._layer_to_run = copy.deepcopy(fp32_layer) # Don't accidentally mess with the original. Make a copy\n",
        "        self.int8_weight = copy.deepcopy(fp32_layer.weight.data)\n",
        "        self.int8_bias = copy.deepcopy(fp32_layer.bias.data)\n",
        "\n",
        "        # Pseudo-quantizes the weights and biases, and stores scale factors and zero points\n",
        "        # Pass in self.int8_weight to quantize_tensor() as the input tensor (first argument)\n",
        "        self.int8_weight, self.weight_scalefactor, self.weight_zeropoint =  #TODO call quantize_tensor() for the weight\n",
        "        # Pass in self.int8_bias to quantize_tensor() as the input tensor (first argument)\n",
        "        self.int8_bias, self.bias_scalefactor, self.bias_zeropoint =  #TODO call quantize_tensor() for the bias\n",
        "\n",
        "    def run_quantized_layer(self, x_int8:torch.Tensor, x_scalefactor: float, x_zeropoint: int, output_scalefactor: float, output_zeropoint: int) -> torch.Tensor:\n",
        "        \"\"\"Runs the layer with a given input, quantized to integer-range [0, 255].\n",
        "\n",
        "        The function first applies the quantization formula to x_int8, int8_weight, int8_bias.  \n",
        "\n",
        "        Using `output_scalefactor` and `output_zeropoint`, the output tensor is quantized using the quantization formula:\n",
        "\n",
        "        `quantized_tensor = zeropoint + fp32_tensor / scalefactor`\n",
        "\n",
        "        Args:\n",
        "            x_int8 (torch.Tensor): Input tensor, with values in range [0, 255].\n",
        "            x_scalefactor (float): Scale factor for the input.\n",
        "            x_zeropoint (int): Zero point for the input.\n",
        "            output_scalefactor (float): Scale factor for the output.\n",
        "            output_zeropoint (int): Zero point for the output.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Output tensor, in integer range [0, 255].\n",
        "        \"\"\"\n",
        "\n",
        "        # Apply the above quantization formula to input, weight, and bias to convert back to fp32\n",
        "        x = x_scalefactor * (x_int8 - x_zeropoint) # Apply the quantization formula to the input\n",
        "        # Use self.int8_weight, self.weight_scalefactor, self.weight_zeropoint\n",
        "        # You do not need to use the variable 'x' for this line\n",
        "        weight =  # TODO: Apply the quantization formula to the weight\n",
        "        # Use self.int8_weight, self.weight_scalefactor, self.weight_zeropoint\n",
        "        # You do not need to use the variable 'x' for this line\n",
        "        bias =  # TODO: Apply the quantization formula to the bias\n",
        "\n",
        "        # Load up the layer with the re-scaled weights and biases (real hardware implements this differently)\n",
        "        self._layer_to_run.weight.data = weight\n",
        "        self._layer_to_run.bias.data = bias\n",
        "\n",
        "        # Apply the quantization formula to the output\n",
        "        int8_output = # TODO: Call self._layer_to_run(x) to retrieve fp32_output, and then quantize the output using 'output_scalefactor' and 'output_zeropoint'\n",
        "        return int8_output\n",
        "\n",
        "class QuantizerForClassyClassifier():\n",
        "    \"\"\"Applies pseudo-quantization to the ClassyClassifier for a given calibration dataset.\n",
        "    \"\"\"\n",
        "    def __init__(self, fp32_model: ClassyClassifier, calibration_dataloader: DataLoader):\n",
        "        \"\"\"Constructor for a QuantizerForClassyClassifier for a given calibration dataset.\n",
        "\n",
        "        Args:\n",
        "            fp32_model (ClassyClassifier): Model to quantize.\n",
        "            calibration_dataloader (DataLoader): Calibration dataset to use.\n",
        "        \"\"\"\n",
        "        self.fp32_model = copy.deepcopy(fp32_model) # Don't accidentally mess with the original. Make a copy\n",
        "        \n",
        "        # Quantize weights and biases\n",
        "        self.int8_layer1_conv = QuantizedLayer(fp32_model.layer1_conv)\n",
        "        self.int8_layer3_conv = QuantizedLayer(fp32_model.layer3_conv)\n",
        "        self.int8_layer6_fc = QuantizedLayer(fp32_model.layer6_fc)\n",
        "        self.int8_layer7_fc = QuantizedLayer(fp32_model.layer7_fc)\n",
        "        self.int8_layer8_fc = QuantizedLayer(fp32_model.layer8_fc)\n",
        "        \n",
        "        # Calibration\n",
        "        self.calibration_dataloader = calibration_dataloader\n",
        "\n",
        "        # Set up calibration stat-tracking\n",
        "        self.calibration_input_stats = {\n",
        "            \"layer1_conv\": {\n",
        "                \"mins\": [],\n",
        "                \"maxes\": [],\n",
        "                \"avg_min\": 0,\n",
        "                \"avg_max\": 0,\n",
        "            },\n",
        "            \"layer3_conv\": {\n",
        "                \"mins\": [],\n",
        "                \"maxes\": [],\n",
        "                \"avg_min\": 0,\n",
        "                \"avg_max\": 0,\n",
        "                \"input_scalefactor\": 0.,\n",
        "                \"input_zeropoint\": 0\n",
        "            },\n",
        "            \"layer6_fc\": {\n",
        "                \"mins\": [],\n",
        "                \"maxes\": [],\n",
        "                \"avg_min\": 0,\n",
        "                \"avg_max\": 0,\n",
        "                \"input_scalefactor\": 0.,\n",
        "                \"input_zeropoint\": 0\n",
        "            },\n",
        "            \"layer7_fc\": {\n",
        "                \"mins\": [],\n",
        "                \"maxes\": [],\n",
        "                \"avg_min\": 0,\n",
        "                \"avg_max\": 0,\n",
        "                \"input_scalefactor\": 0.,\n",
        "                \"input_zeropoint\": 0\n",
        "            },\n",
        "            \"layer8_fc\": {\n",
        "                \"mins\": [],\n",
        "                \"maxes\": [],\n",
        "                \"avg_min\": 0,\n",
        "                \"avg_max\": 0,\n",
        "                \"input_scalefactor\": 0.,\n",
        "                \"input_zeropoint\": 0\n",
        "            },\n",
        "\n",
        "        }\n",
        "\n",
        "        # Calibrate\n",
        "        self.calibrate_with_dataloader()\n",
        "\n",
        "    def calibrate_with_dataloader(self): \n",
        "        \"\"\"Inserts observers into the 32-bit float forward pass of the ClassyClassifier, and then runs the calibration dataset.\n",
        "\n",
        "        Records average input mins/maxes for each Conv2d and Linear layer, and uses them to calculate input scale factors and zero points.\n",
        "\n",
        "        Stores into `self.calibration_input_stats`.\n",
        "        \"\"\"\n",
        "        with torch.no_grad():\n",
        "            for x, _ in self.calibration_dataloader:\n",
        "                batch_size = x.shape[0]\n",
        "                \n",
        "                ### Refer to the forward() function of ClassyClassifier to understand the order of layers and relu() activation functions used below\n",
        "                ### Note that 'self.calibration_input_stats' is a dictionary, and is defined above in the constructor for the class QuantizerForClassyClassifier\n",
        "                ### The string literals \"layer1_conv\" and \"layer3_conv\" used to index 'self.calibration_input_stats' correspond with the 'layer1_conv' and 'layer3_conv' in ClassyClassifier\n",
        "                ### Use this information to fill out the statements for the remaining three layers\n",
        "\n",
        "                # Get calibration data going into layer 1, then use ReLU activations and pooling, store to self.calibration_input_stats\n",
        "                # collect the min and maxes stats for the input of layer 1 (aka the input x)\n",
        "                self.calibration_input_stats[\"layer1_conv\"][\"mins\"].append(x.view(batch_size, -1).min(dim=1)[0].sum().item())\n",
        "                self.calibration_input_stats[\"layer1_conv\"][\"maxes\"].append(x.view(batch_size, -1).max(dim=1)[0].sum().item())\n",
        "                # use ReLU\n",
        "                x = F.relu(self.fp32_model.layer1_conv(x))\n",
        "                # do layer-2 pooling\n",
        "                x = self.fp32_model.layer2_pool(x)\n",
        "\n",
        "                # Get calibration data going into layer 3, then use ReLU activations and pooling, like above, store to self.calibration_input_stats               \n",
        "                # Collect the min and maxes stats for the input of layer 3 (aka the output of layer 2)\n",
        "                # Use ReLU, do layer-4 pooling\n",
        "                self.calibration_input_stats[\"layer3_conv\"][\"mins\"].append(x.view(batch_size, -1).min(dim=1)[0].sum().item())\n",
        "                self.calibration_input_stats[\"layer3_conv\"][\"maxes\"].append(x.view(batch_size, -1).max(dim=1)[0].sum().item())\n",
        "                x = F.relu(self.fp32_model.layer3_conv(x))\n",
        "                x = self.fp32_model.layer4_pool(x)\n",
        "                x = self.fp32_model.layer5_flat(x)\n",
        "\n",
        "                # Get calibration data going into layer 6, then use ReLU activations, like above, store to self.calibration_input_stats\n",
        "                # TODO: collect the min and maxes stats for the input of layer 6 (aka the output of layer 5)\n",
        "                # TODO: use ReLU\n",
        "                \n",
        "\n",
        "                # Get calibration data going into layer 7, then use ReLU activations, like above, store to self.calibration_input_stats\n",
        "                # TODO: collect the min and maxes stats for the input of layer 7 (aka the output of layer 6)\n",
        "                # TODO: use ReLU\n",
        "                \n",
        "                \n",
        "                # Get calibration data going into layer 8, like above, store to self.calibration_input_stats\n",
        "                # TODO: collect the min and maxes stats for the input of layer 7 (aka the output of layer 6)\n",
        "                # No need to do ReLU and finish the forward pass because there are no further stats to collect\n",
        "                \n",
        "\n",
        "\n",
        "        # Convert the lists of mins and maxes into averages, layer-input scalefactors, and layer-input zeropoints\n",
        "        for layer_name in self.calibration_input_stats:\n",
        "            avg_min = statistics.mean(self.calibration_input_stats[layer_name][\"mins\"])\n",
        "            avg_max = statistics.mean(self.calibration_input_stats[layer_name][\"maxes\"])\n",
        "            # Use the two variables 'avg_min' and 'avg_max' to compute the scalefactor and zeropoint\n",
        "            input_scalefactor, input_zeropoint = # TODO: call calculate_scalefactor_and_zeropoint()\n",
        "            self.calibration_input_stats[layer_name][\"avg_min\"] = avg_min\n",
        "            self.calibration_input_stats[layer_name][\"avg_max\"] = avg_max\n",
        "            self.calibration_input_stats[layer_name][\"input_scalefactor\"] = input_scalefactor \n",
        "            self.calibration_input_stats[layer_name][\"input_zeropoint\"] = input_zeropoint\n",
        "    \n",
        "    def run_calibrated_quantized(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Runs the pseudo-quantized ClassyClassifier on a 32-bit float tensor.\n",
        "        \n",
        "        Quantizes the tensor to [0, 255] prior to forward pass, and then dequantizes back to float before return.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): Input 32-bit float tensor.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Output 32-bit float tensor.\n",
        "        \"\"\"\n",
        "        x = copy.deepcopy(x)\n",
        "\n",
        "        # Use the calibration input stats for Layer 1 to quantize the tensor and run the layer\n",
        "        x, x_scalefactor, x_zeropoint = quantize_tensor(x, self.calibration_input_stats[\"layer1_conv\"][\"avg_min\"], self.calibration_input_stats[\"layer1_conv\"][\"avg_max\"])\n",
        "        # The \\ in Python continues the statement on the next line - it is often used to make long, single line statements more readable\n",
        "        x = self.int8_layer1_conv.run_quantized_layer(x, x_scalefactor, x_zeropoint, \\\n",
        "                                                      self.calibration_input_stats[\"layer3_conv\"][\"input_scalefactor\"], self.calibration_input_stats[\"layer3_conv\"][\"input_zeropoint\"])\n",
        "        x = F.relu(x)\n",
        "        x = self.fp32_model.layer2_pool(x)\n",
        "\n",
        "        # TODO (worth 13 points): As above, use the calibration input stats for the appropriate layers to complete the rest of the forward pass: layer 4-7 (DON'T DO layer 8 yet)\n",
        "        # Remember to look at the order of layers and activations in calibrate_with_dataloader() \n",
        "        # Don't forget to do your ReLU activations and pooling as necessary!\n",
        "        # Notice the pattern between the first two and last two parameters in calls to run_quantized_layer()\n",
        "        x = self.int8_layer3_conv.run_quantized_layer(x, self.calibration_input_stats[\"layer3_conv\"][\"input_scalefactor\"], self.calibration_input_stats[\"layer3_conv\"][\"input_zeropoint\"], \\\n",
        "                                                      self.calibration_input_stats[\"layer6_fc\"][\"input_scalefactor\"], self.calibration_input_stats[\"layer6_fc\"][\"input_zeropoint\"])\n",
        "        x = F.relu(x)\n",
        "      \n",
        "        x = # TODO: dequantize the tensor 'x' using dequantize_tensor(), the Layer 8 scalefactor, and zeropoint\n",
        "        \n",
        "        # Run the last layer in FP32, dequantized\n",
        "        x = self.fp32_model.layer8_fc(x)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y2bgybbDLQ4V"
      },
      "source": [
        "# Just one line to set everything up using the test dataloader as the calibration dataset\n",
        "quantizer = QuantizerForClassyClassifier(fp32_classifier.eval(), test_dataloader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nvP4exwNLQ4X"
      },
      "source": [
        "dataiter = iter(test_dataloader)\n",
        "first_batch_of_imgs, first_batch_of_groundtruth_labels = dataiter.next()\n",
        "\n",
        "print(\"SAMPLE IMAGES from CIFAR-10:\")\n",
        "display_cifar10_imgs(torchvision.utils.make_grid(first_batch_of_imgs))\n",
        "print(\"CORRESPONDING GROUND TRUTH LABELS:\")\n",
        "print(\"    \" + \"     \".join([CIFAR10_CLASS_NAMES[idx] for idx in first_batch_of_groundtruth_labels]))\n",
        "\n",
        "outputs = quantizer.run_calibrated_quantized(first_batch_of_imgs)\n",
        "\n",
        "_, predicted = torch.max(outputs, 1)\n",
        "print(\"PREDICTED CLASSES from PSEUDO-QUANTIZED NETWORK should match original neural network outputs (i.e., cat, car, plane, plane, frog):\")\n",
        "print(\"    \" + \"     \".join([CIFAR10_CLASS_NAMES[idx] for idx in predicted]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DkpD1dB2LQ4X"
      },
      "source": [
        "# Quantizing with Torch.FX (10 pts)\n",
        "\n",
        "Torch.FX is a library in PyTorch that represents the computational graph of a neural network. It also has a built-in API that makes quantization a snap!\n",
        "\n",
        "> **TASKS:**\n",
        "> 1. Read and run the cells below. You should see that the Torch.FX-quantized model is much smaller (160kB) than the original model (540kB). You should also see that accuracy remains similar.\n",
        "> 2. Answer the questions.\n",
        "\n",
        "**(2 pts) Concept Question 1: Explain what calibration does.**\n",
        "\n",
        "_[your answer here]_\n",
        "\n",
        "**(2 pts) Concept Question 2: Why might the accuracy of a quantized network be _better_ than that of the original? Doesn't quantization cause the network to lose precision?**\n",
        "\n",
        "_[your answer here]_\n",
        "\n",
        "**(6 pts) Ensure the accuracy of the Torch.FX model and your Pseudo-Quantized Model is within +- 0.08 of the original, non-quantized model**. If your accuracy is not within that range, you most likely have an incorrect implementation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nbmx63LBLQ4Y"
      },
      "source": [
        "import os\n",
        "from torch.quantization import get_default_qconfig\n",
        "from torch.quantization.quantize_fx import prepare_fx, convert_fx, QuantizedGraphModule\n",
        "\n",
        "fp32_classifier.eval()\n",
        "qconfig = get_default_qconfig(\"fbgemm\")\n",
        "qconfig_dict = {\"\": qconfig}\n",
        "\n",
        "# Fuse modules and insert observers to automatically collect min/max during calibration\n",
        "prepared_model = prepare_fx(fp32_classifier, qconfig_dict)  \n",
        "\n",
        "# Calibrate\n",
        "with torch.no_grad():\n",
        "    for x, _ in test_dataloader:\n",
        "        prepared_model(x)\n",
        "\n",
        "# Convert the calibrated model to a quantized model\n",
        "quantized_model = convert_fx(prepared_model).eval()\n",
        "print(\"FX-QUANTIZED MODEL:\")\n",
        "print(quantized_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XbooCHE9LQ4Y"
      },
      "source": [
        "dataiter = iter(test_dataloader)\n",
        "first_batch_of_imgs, first_batch_of_groundtruth_labels = dataiter.next()\n",
        "\n",
        "print(\"SAMPLE IMAGES from CIFAR-10:\")\n",
        "display_cifar10_imgs(torchvision.utils.make_grid(first_batch_of_imgs))\n",
        "print(\"CORRESPONDING GROUND TRUTH LABELS:\")\n",
        "print(\"    \" + \"     \".join([CIFAR10_CLASS_NAMES[idx] for idx in first_batch_of_groundtruth_labels]))\n",
        "\n",
        "outputs = quantized_model(first_batch_of_imgs)\n",
        "_, predicted = torch.max(outputs, 1)\n",
        "\n",
        "print(\"PREDICTED CLASSES from FX-QUANTIZED NETWORK should match be similar to original neural network outputs (i.e., cat, car, plane, plane, bird):\")\n",
        "print(\"    \" + \"     \".join([CIFAR10_CLASS_NAMES[idx] for idx in predicted]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M06YUgRgLQ4Y"
      },
      "source": [
        "def evaluate_ClassyClassifier(model: Union[ClassyClassifier, QuantizerForClassyClassifier, QuantizedGraphModule], test_dataloader: DataLoader, report_model_size:bool=False):\n",
        "    num_images_correct = 0\n",
        "    total_images_seen = 0\n",
        "    timings = []\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_dataloader:\n",
        "            batch_size = images.shape[0]\n",
        "            \n",
        "            if type(model) is QuantizerForClassyClassifier:\n",
        "                outputs = model.run_calibrated_quantized(images)\n",
        "            else:\n",
        "                outputs = model(images)\n",
        "            \n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            \n",
        "            total_images_seen += batch_size\n",
        "            \n",
        "            num_images_correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print(\"Accuracy: {0:.3f}, {1} correct/{2} total images\".format(num_images_correct/total_images_seen, num_images_correct, total_images_seen))\n",
        "    if report_model_size and type(model) is not QuantizerForClassyClassifier:\n",
        "        torch.jit.save(torch.jit.script(model), \"temp.p\")\n",
        "        print(\"Model Size (kB): {0:.1f}\".format(os.path.getsize(\"temp.p\")/1024))\n",
        "        os.remove(\"temp.p\")\n",
        "\n",
        "print(\"ACCURACY:\")\n",
        "\n",
        "print(\"ORIGINAL ClassyClassifier:\")\n",
        "evaluate_ClassyClassifier(fp32_classifier, test_dataloader, True)\n",
        "\n",
        "print(\"FX-QUANTIZED ClassyClassifier:\")\n",
        "evaluate_ClassyClassifier(torch.jit.script(quantized_model).eval(), test_dataloader, True)\n",
        "\n",
        "print(\"PSEUDO-QUANTIZED ClassyClassifier:\")\n",
        "evaluate_ClassyClassifier(quantizer, test_dataloader)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}